<p align="center">
  <img src="assets/banner-intercom-dataops.png" width="100%" alt="Intercom DataOps Banner"/>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Python-3.10+-blue?logo=python"/>
  <img src="https://img.shields.io/badge/DataOps-Pipeline-orange"/>
  <img src="https://img.shields.io/badge/Status-Ativo-brightgreen"/>
  <img src="https://img.shields.io/badge/Autor-Jonathan_Sienkiewicz-6aa84f"/>
</p>

# Intercom DataOps â€” Operational Data Pipeline
### Autor: Jonathan Santos de Jesus Sienkiewicz
### Ano: 2025

---

## ğŸ“Œ VisÃ£o Geral

Este projeto implementa um **pipeline completo de DataOps**, simulando a arquitetura real utilizada em times de operaÃ§Ãµes e suporte de empresas orientadas a dados.

O pipeline cobre todo o fluxo:

- IngestÃ£o de dados
- Limpeza e padronizaÃ§Ã£o
- GeraÃ§Ã£o de mÃ©tricas operacionais
- SÃ©ries temporais
- OrganizaÃ§Ã£o em Data Lake local
- VisualizaÃ§Ã£o via dashboard (Streamlit)

O objetivo Ã© demonstrar **capacidade prÃ¡tica real em Engenharia de Dados e Analytics Engineering**, com foco em:

- ETL profissional
- Data Cleaning robusto
- VisualizaÃ§Ã£o orientada a negÃ³cio
- EstruturaÃ§Ã£o de projetos escalÃ¡veis

âœ… **Importante:** todos os dados utilizados sÃ£o 100% sintÃ©ticos e nÃ£o representam nenhuma empresa real ou dado sensÃ­vel.

---

## ğŸ—ï¸ Arquitetura do Pipeline

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚      Arquivos CSV        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
             IngestÃ£o
                  â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚       data/raw/          â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
          Limpeza + NormalizaÃ§Ã£o
                  â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚    data/processed/       â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
         MÃ©tricas + Time Series
                  â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚     data/analytics/      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
           ExportaÃ§Ã£o Pipeline
                  â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚     data/datalake/       â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

---

## ğŸ“ Estrutura do Projeto

projeto-intercom-dataops/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/               â†’ Dados brutos sintÃ©ticos
â”‚   â”œâ”€â”€ processed/         â†’ Dados limpos
â”‚   â”œâ”€â”€ analytics/
â”‚   â”‚   â”œâ”€â”€ metrics/       â†’ KPIs gerados
â”‚   â”‚   â””â”€â”€ time_series/   â†’ SÃ©ries temporais
â”‚   â””â”€â”€ datalake/          â†’ Data Lake local
â”‚
â”œâ”€â”€ data/cleaning/
â”‚   â””â”€â”€ clean_tickets.py
â”‚
â”œâ”€â”€ data/analytics/
â”‚   â”œâ”€â”€ metrics.py
â”‚   â””â”€â”€ time_series.py
â”‚
â”œâ”€â”€ data/export/
â”‚   â””â”€â”€ export_to_datalake.py
â”‚
â”œâ”€â”€ dashboard/
â”‚   â””â”€â”€ app.py
â”‚
â”œâ”€â”€ assets/
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

---

## ğŸš€ Sprints TÃ©cnicos

Sprint 1 â€” Setup & IngestÃ£o  
CriaÃ§Ã£o da estrutura do projeto e ingestÃ£o de dados sintÃ©ticos.

Sprint 2 â€” Data Cleaning  
PadronizaÃ§Ã£o de colunas, limpeza de dados e validaÃ§Ãµes automÃ¡ticas.

Sprint 3 â€” KPIs  
GeraÃ§Ã£o de mÃ©tricas operacionais por canal, categoria e time.

Sprint 4 â€” Time Series  
CriaÃ§Ã£o de sÃ©ries temporais para anÃ¡lise de volume.

Sprint 5 â€” Data Lake  
OrganizaÃ§Ã£o final dos dados no Data Lake local.

Sprint 6 â€” Dashboard  
Dashboard interativo com Streamlit e Plotly.

---

## ğŸ“Š Dashboard (Sprint 6)

O projeto inclui um painel interativo construÃ­do com **Streamlit**.

Para executar localmente:

    streamlit run dashboard/app.py

---

## ğŸ§° Tecnologias Utilizadas

- Python
- Pandas
- NumPy
- Faker
- Streamlit
- Plotly

---

## ğŸ“¦ InstalaÃ§Ã£o

    pip install -r requirements.txt

---

## ğŸ“œ PolÃ­tica de Dados

Todos os dados utilizados sÃ£o **sintÃ©ticos**, gerados artificialmente para fins educacionais.  
NÃ£o hÃ¡ uso de informaÃ§Ãµes reais, sensÃ­veis ou corporativas.

---

## ğŸ“Œ Objetivo Profissional

Este projeto foi desenvolvido com foco em demonstrar capacidade prÃ¡tica para vagas de:

- Analista de Dados
- Analytics Engineer
- Engenharia de Dados (nÃ­vel jÃºnior/pleno)

---

## âœ… Status

Projeto completo, funcional e pronto para demonstraÃ§Ã£o tÃ©cnica.

---

Autor: Jonathan Santos de Jesus Sienkiewicz
