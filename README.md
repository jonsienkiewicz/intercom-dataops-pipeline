<p align="center">
  <img src="assets/banner-intercom-dataops.png" width="100%" alt="Intercom DataOps Banner"/>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Python-3.10+-blue?logo=python"/>
  <img src="https://img.shields.io/badge/DataOps-Pipeline-orange"/>
  <img src="https://img.shields.io/badge/Status-Ativo-brightgreen"/>
  <img src="https://img.shields.io/badge/Autor-Jonathan_Sienkiewicz-6aa84f"/>
</p>

# Intercom DataOps â€” Pipeline de Dados Operacionais  
### Autor: Jonathan Santos de Jesus Sienkiewicz  
### PerÃ­odo: 2025  
---

Este projeto implementa um pipeline completo de **DataOps**, desde a ingestÃ£o de dados exportados da Intercom atÃ© a geraÃ§Ã£o de mÃ©tricas, sÃ©ries temporais, anÃ¡lises operacionais e exportaÃ§Ã£o consolidada para um **Data Lake local**.

O objetivo Ã© demonstrar domÃ­nio prÃ¡tico em:

- `ETL / Data Cleaning`
- `MÃ©tricas Operacionais`
- `Time Series`
- `Design de Pipeline`
- `Data Lake`
- OrganizaÃ§Ã£o do projeto com boas prÃ¡ticas de Engenharia de Dados

---

# Arquitetura do Pipeline

(ASCII diagram â€” use como referÃªncia visual)

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚       Intercom CSVs     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
            IngestÃ£o Manual
                  â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚       data/raw/         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
       Limpeza e PadronizaÃ§Ã£o
                  â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚    data/processed/      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
         MÃ©tricas e Time Series
                  â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚    data/analytics/      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
            ExportaÃ§Ã£o Final
                  â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚     data/datalake/      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

---

# ğŸ“ Estrutura Geral do Projeto

`projeto-intercom-dataops/`

- `data/`
  - `raw/`               â†’ CSVs originais da Intercom
  - `processed/`         â†’ Dados limpos e padronizados
  - `analytics/`
    - `metrics/`         â†’ MÃ©tricas operacionais
    - `time_series/`     â†’ SÃ©ries temporais
  - `datalake/`          â†’ Data Lake local consolidado
- `data/cleaning/`       â†’ Scripts de limpeza
  - `clean_tickets.py`
- `data/analytics/`      â†’ Scripts analÃ­ticos
  - `metrics.py`
  - `time_series.py`
- `data/export/`
  - `export_to_datalake.py`
- `assets/`              â†’ Banner e imagens
- `requirements.txt`
- `README.md`

---

# ğŸš€ Sprints do Projeto

## Sprint 1 â€” ConfiguraÃ§Ã£o Inicial & IngestÃ£o
- OrganizaÃ§Ã£o da estrutura do repositÃ³rio.
- PadronizaÃ§Ã£o de diretÃ³rios no padrÃ£o DataOps.
- IngestÃ£o manual dos CSVs exportados da Intercom em `data/raw/`.
- CriaÃ§Ã£o do repositÃ³rio Git e branch de desenvolvimento.

## Sprint 2 â€” Data Cleaning (Limpeza e PadronizaÃ§Ã£o)
**Arquivo principal:** `data/cleaning/clean_tickets.py`

Principais passos executados:
- NormalizaÃ§Ã£o de nomes de colunas (snake_case).
- ConversÃ£o rigorosa de tipos (datas, numÃ©ricos, strings).
- PadronizaÃ§Ã£o de datas com `pd.to_datetime(..., errors='coerce')`.
- RemoÃ§Ã£o de ruÃ­do textual: valores como `"nan"`, `"None"`, `"<NA>"`.
- Trim de campos textuais (`.str.strip()`).

Regra aplicada (exemplo):
- `df[c] = df[c].astype("string").str.strip().replace({"nan": None, "None": None, "<NA>": None})`

VerificaÃ§Ãµes automÃ¡ticas:
- Checagem de colunas esperadas.
- IdentificaÃ§Ã£o de campos crÃ­ticos vazios.
- ValidaÃ§Ã£o de datas.

**Output:** `data/processed/all_tickets_clean.csv`

## Sprint 3 â€” MÃ©tricas Operacionais (KPIs)
**Arquivo:** `data/analytics/metrics.py`

KPIs implementados:
- `total_tickets`
- `tickets por categoria`
- `tickets por canal`
- `tickets por time responsÃ¡vel`
- ValidaÃ§Ã£o da coluna `ticket_time_to_resolve_(seconds)` (existÃªncia e completude)

**Output:** arquivos em `data/analytics/metrics/`  
Exemplos:
- `total_tickets.txt`
- `tickets_por_categoria.csv`
- `tickets_por_canal.csv`
- `tickets_por_time.csv`

## Sprint 4 â€” SÃ©ries Temporais (Time Series)
**Arquivo:** `data/analytics/time_series.py`

Gera:
- `tickets_por_dia.csv`
- `tickets_por_mes.csv`
- `tempo_resolucao_por_mes.csv` (quando dados de resoluÃ§Ã£o disponÃ­veis)

**Output:** `data/analytics/time_series/`

## Sprint 5 â€” ExportaÃ§Ã£o para Data Lake Local
**Arquivo:** `data/export/export_to_datalake.py`

Funcionalidade:
- Replica `processed/` e `analytics/` para `data/datalake/`
- MantÃ©m hierarquia e nomes
- Simula um Data Lake (prÃ©via local para S3/GCS)

Estrutura final gerada:
- `data/datalake/processed/`
- `data/datalake/analytics/metrics/`
- `data/datalake/analytics/time_series/`

---

# ğŸ Status Atual do Projeto

- Pipeline funcional de ponta a ponta
- Dados limpos e padronizados
- MÃ©tricas consolidadas
- SÃ©ries temporais geradas
- Data Lake local atualizado
- PrÃ³ximo objetivo: **Sprint 6 â€” Dashboard & IntegraÃ§Ã£o com API da Intercom**

---

# ğŸ“¦ Requisitos

Coloque no `requirements.txt`:

- `pandas`
- `numpy`
- `python-dotenv`

(Instale com `pip install -r requirements.txt`)

---

# ğŸ“Œ PrÃ³ximos Passos (Sprint 6+)

- Criar dashboard (Looker Studio / Streamlit / PowerBI)
- IngestÃ£o automÃ¡tica pela API da Intercom
- Modelagem dimensional (fato + dimensÃµes)
- Automatizar pipeline (Cron / Airflow / Prefect)
- Alerts e monitoramento (Slack / Prometheus / Grafana)

---

# ğŸ“œ LicenÃ§a

Projeto de estudo e demonstraÃ§Ã£o tÃ©cnica.  
Uso livre para fins educacionais.

Todos os datasets utilizados neste repositÃ³rio sÃ£o integralmente sintÃ©ticos, criados apenas para fins acadÃªmicos e nÃ£o representam dados reais de usuÃ¡rios, clientes, empresas ou sistemas. NÃ£o hÃ¡ qualquer dado sensÃ­vel neste repositÃ³rio.